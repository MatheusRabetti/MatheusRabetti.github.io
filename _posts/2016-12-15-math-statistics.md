# Statistics is not math

“Here is a column of a couple of dozen numbers. From them, calculate the mean and standard deviation. 
When you are finished — it should take you a good fifteen to twenty minutes — report back to me.”

So goes the instruction in many, if not most or even all, undergraduate statistics courses across the land.
Running through endless examples of plugging numbers into calculators and pressing certain buttons.
So what if the students forget why they’ve done it?

This inertia is a quirk of human nature and is common in any field of instruction: its limitations are
overcome easily by all serious students. Far more restraining, however, are the pernicious effects of the belief
that statistics is a branch of mathematics.

Statistics is not math; neither is probability. It is true that math has proven unreasonably effective
in understanding statistics, but it is not, or at least not the sole, language to describe its workings. 

That language is philosophical. Just think: statistics self-named purpose is to compile evidence to use in quantifying 
uncertainty in (self-selected) hypotheses. How this evidence bears on the hypotheses may be best described mathematically,
but why it does so cannot be. It also cannot be that because statistics uses so much math that it is math. 

To master probability and statistics requires mastering a great chunk of math. But we begin to go wrong when we 
mindlessly apply equations in inappropriate situations because of the allure of quantification. **Equations become a scapegoat!**

Philosophy sharpens the mind. It teaches us to recognize and eliminate sloppy thinking and writing,
two elements rife in our field. 
If people spent more time thinking about what they are saying and doing, much error would be reduced or eliminated. 

I’ll give just one example. What is a confidence interval? It is an equation, of course, 
that will provide you an interval for your data. It is meant to provide a measure of the uncertainty of a parameter estimate. 
Now, strictly according to frequentist theory the only thing you can say about the CI
you have in hand is that the true value of the parameter lies within it or that it does not. 
This is a tautology, therefore it is always true. Thus, the CI provides no measure of uncertainty at all. 

> Probability is not decision!

But ask your neighborhood statistician and you will hear words about “95% confidence”, about “long runs”, 
about “other experiments”, etc., etc. **These poorly chosen phrases are a bar to clear thinking**. 
They make the utterer forget that all he can say is some tautological, and therefore trivial, truth. 
He has concentrated on the math, making sure to divide by n minus one in the appropriate place, etc.,
and has not given any time to consider why the calculation exists.

Statistics is all about understanding data – numbers with context and meaning. A computer can do all of the calculations and all of the numerical work with finding a mean, a standard deviation, and even a confidence interval (all things we do in statistics). But, only a person can tell you if the mean really describes the data set or what the confidence interval is actually telling us.

So, statistics is about taking the information we get from mathematics and interpreting it. You may look at the math behind the information, but only to get a better idea of how to make a decision.

# Statistics beyond the math

I call myself a statistician, because, well, I’m a statistics graduate student. However, ask me specific questions about hypothesis tests or required sampling size, and my answer probably won’t be very good.

The other day I was trying to think of the last time I did an actual hypothesis test or formal analysis. I couldn’t remember. I actually had to dig up old course listings to figure out when it was. It was four years ago during my first year of graduate school. I did well in those courses, and I’m confident I could do that stuff with a quick refresher, but it’s a no go off the cuff. It’s just not something I do regularly.

Instead, the most important things I’ve learned are less formal, but have proven extremely useful when working/playing with data. Here they are in no particular order.

## Attention to Detail

Oftentimes it’s the little things that end up being the most important. There was this one time in class when my professor put up a graph on the projector. It was a bunch of data points with a smooth fitted line. He asked what we saw. Well, there was an increase in the beginning, a leveling off in the middle, and then another increase. However, what I missed was the little blip in the curve in the first increase. That was what we were after.

The point is that trends and patterns are important, but so are outliers, missing data points, and inconsistencies.

## See the Big Picture

With that said, it’s important not to get too caught up with individual data points or a tiny section in a really big dataset. 

## No Agendas

This should go without saying, but approach data as objectively as possible. I’m not saying you shouldn’t have a hunch about what you’re looking for, but don’t let your preconceived ideas influence the results. Because if you go to length looking for some specific pattern, you’re probably going to find it. It’ll just be at the sacrifice of accurate results.

## Look Outside the Data

Context, context, context. Sometimes this will come in the form of metadata. Other times it’ll come from more data.

The more you know about how the data was collected, where it came from, when it happened, and what was going on at the time, the more informative your results and the more confident you can be about your findings.

## Ask Why

Finally, and this is the most important thing I’ve learned, always ask why. When you see a blip in a graph, you should wonder why it’s there. If you find some correlation, you should think about whether or not it makes any sense. If it does make sense, then cool, but if not, dig deeper. Numbers are great, but you have to remember that when humans are involved, errors are always a possibility.

